---
layout: single
title: "데이터 정제, 변형"
date: "2023-11-21 11:00:00 +0900"
last_modified_at: "2023-11-21 12:00:00 +0900"
author_profile: true
sidebar:
  nav: "sidebar-category"
categories:
  - Python
---

DB 모델링을 위한 데이터 정비<br>

**미리 요점 정리**<br>
결측값 처리에 쓰이는 매서드 isna(), dropna()<br>
결측값 메우는 메서드 fillna()<br>
NaN값으로 변환 numpy.nan
 

## 누락된 데이터 처리

판다스의 모든 통계 처리는 누락된 데이터를 배제하고 처리한다<br>
판다스 객체에서 누락된 값을 결측값(missing value), 감싯값(sentinel value)라 부른다<br>
float64 dtype 가지는 데이터의 경우 판다스 실숫값인 NaN으로 누락된 데이터를 표시

```python
# 누락된 데이터 처리

In [26]: import numpy as np
In [16]: import pandas as pd

In [27]: float_data = pd.Series([1.2, -3.5, np.nan, 0])

In [28]: float_data
Out[28]:
0    1.2
1   -3.5
2    NaN
3    0.0
dtype: float64 # 판다스는 NaN 실숫값이 있다 (숫자가 아니다 라는 의미)

In [29]: float_data.isna() 
Out[29]:
0    False
1    False
2     True
3    False
dtype: bool

In [30]: string_data = pd.Series(['aardvark', np.nan, None, 'avocada'])

In [31]: string_data
Out[31]:
0    aardvark
1         NaN
2        None
3     avocada
dtype: object

In [32]: string_data.isna()
Out[32]:
0    False
1     True
2     True
3    False
dtype: bool

In [33]: float_data = pd.Series([1, 2, None], dtype='float64')

In [34]: float_data
Out[34]:
0    1.0
1    2.0
2    NaN
dtype: float64

In [35]: float_data.isna()
Out[35]:
0    False
1    False
2     True
dtype: bool
```
## 누락된 데이터 골라내기
pandas.isna 나 불리언 색인을 사용해 누락된 데이터를 골라낼 수 있지만 더 쉬운 방법이 있다<br><br>
**dropna 매서드 사용하기**
```python
# Series에서 dropna 적용시 널이 아닌 데이터, 색인값 만 들어 있는 Series가 반환된다

In [36]: data = pd.Series([1, np.nan, 3.5, np.nan, 7])

In [37]: data.dropna()
Out[37]:
0    1.0
2    3.5
4    7.0
dtype: float64

# dropna 매서드는 아래와 같은 작동이다

In [38]: data[data.notna()]
Out[38]:
0    1.0
2    3.5
4    7.0
dtype: float64
```
```python
# DataFrame 객체일 경우

# 연습할 DF생성
In [39]: data = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan], [np.nan, np.nan, np.nan], [np.nan,
    ...:  6.5, 3.]])

In [40]: data
Out[40]:
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
2  NaN  NaN  NaN
3  NaN  6.5  3.0

In [41]: data.dropna() # NA 들어있는 행 제외 # 기본적으로 NA값이 하나라도 포함되어 있으면 해당 행을 제외한다
Out[41]:
     0    1    2
0  1.0  6.5  3.0

In [42]: data.dropna(how='all') # 모든 값이 NA인 행만 제외
Out[42]:
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
3  NaN  6.5  3.0
```
```python
In [43]: data[4] = np.nan

In [44]: data
Out[44]:
     0    1    2   4
0  1.0  6.5  3.0 NaN
1  1.0  NaN  NaN NaN
2  NaN  NaN  NaN NaN
3  NaN  6.5  3.0 NaN

In [45]: data.dropna(axis='columns', how='all') # 축방향을 열방향으로, 모든 값이 null일때 제외
Out[45]:
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
2  NaN  NaN  NaN
3  NaN  6.5  3.0

In [46]: data.dropna(how='all') # 기본은 행방향이라 지정안해도 됨. axis='index'
Out[46]:
     0    1    2   4
0  1.0  6.5  3.0 NaN
1  1.0  NaN  NaN NaN
3  NaN  6.5  3.0 NaN

# 제외한 상태의 배열에서 추가로 4열 지우기

In [47]: data.dropna(how='all').dropna(axis='columns', how='all')
Out[47]:
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
3  NaN  6.5  3.0
```
```python
In [49]: df = pd.DataFrame(np.random.standard_normal((7, 3)))
# Numpy 라이브러리의 random 모듈의 standard_normal 함수
# 표준정규분포(standard normal distribution)에서 샘플링한 난수를 반환합니다.


In [50]: df
Out[50]:
          0         1         2
0  1.915663 -1.122603  0.394731
1  0.948865 -0.825860 -0.526707
2  0.397456  2.044012  0.153846
3 -1.014162 -0.025973 -1.557946
4  0.578994 -0.091222  1.923340
5 -0.370613  1.310512 -2.062977
6  0.307368 -0.110701 -0.554373

In [51]: df.iloc[:4, 1] = np.nan

In [52]: df
Out[52]:
          0         1         2
0  1.915663       NaN  0.394731
1  0.948865       NaN -0.526707
2  0.397456       NaN  0.153846
3 -1.014162       NaN -1.557946
4  0.578994 -0.091222  1.923340
5 -0.370613  1.310512 -2.062977
6  0.307368 -0.110701 -0.554373

In [53]: df.iloc[:2, 2] = np.nan

In [54]: df
Out[54]:
          0         1         2
0  1.915663       NaN       NaN
1  0.948865       NaN       NaN
2  0.397456       NaN  0.153846
3 -1.014162       NaN -1.557946
4  0.578994 -0.091222  1.923340
5 -0.370613  1.310512 -2.062977
6  0.307368 -0.110701 -0.554373

In [55]: df.dropna()
Out[55]:
          0         1         2
4  0.578994 -0.091222  1.923340
5 -0.370613  1.310512 -2.062977
6  0.307368 -0.110701 -0.554373

In [56]: df.dropna(thresh=2) # 행에 요소 개수가 2개 미만이면 제외 (NaN은 세지 않음)
Out[56]:
          0         1         2
2  0.397456       NaN  0.153846
3 -1.014162       NaN -1.557946
4  0.578994 -0.091222  1.923340
5 -0.370613  1.310512 -2.062977
6  0.307368 -0.110701 -0.554373
```
## 결측치 채우기
누락된 값을 제외하지 않고 메우고만 싶을때
```python
In [60]: df.fillna(0)
Out[60]:
          0         1         2
0  1.915663  0.000000  0.000000
1  0.948865  0.000000  0.000000
2  0.397456  0.000000  0.153846
3 -1.014162  0.000000 -1.557946
4  0.578994 -0.091222  1.923340
5 -0.370613  1.310512 -2.062977
6  0.307368 -0.110701 -0.554373

In [61]: df.fillna({1:0.5, 2:0}) # 각 열마다 입력한 값이 채워짐
Out[61]:
          0         1         2
0  1.915663  0.500000  0.000000
1  0.948865  0.500000  0.000000
2  0.397456  0.500000  0.153846
3 -1.014162  0.500000 -1.557946
4  0.578994 -0.091222  1.923340
5 -0.370613  1.310512 -2.062977
6  0.307368 -0.110701 -0.554373
```
```python
In [62]: df = pd.DataFrame(np.random.standard_normal((6, 3)))

In [63]: df.iloc[2:, 1] = np.nan

In [64]: df.iloc[4:, 2] = np.nan

In [65]: df
Out[65]:
          0         1         2
0 -0.452703  1.317666 -0.314142
1 -1.202633 -0.762146 -1.283089
2  0.411725       NaN -0.209268
3 -2.426604       NaN  0.261904
4 -0.024134       NaN       NaN
5  1.303691       NaN       NaN

In [66]: df.fillna(method='ffill') # 바로 앞 값으로 채워짐 (열기준)
Out[66]:
          0         1         2
0 -0.452703  1.317666 -0.314142
1 -1.202633 -0.762146 -1.283089
2  0.411725 -0.762146 -0.209268
3 -2.426604 -0.762146  0.261904
4 -0.024134 -0.762146  0.261904
5  1.303691 -0.762146  0.261904

In [67]: df.fillna(method='ffill', limit=2) # 2개만
Out[67]:
          0         1         2
0 -0.452703  1.317666 -0.314142
1 -1.202633 -0.762146 -1.283089
2  0.411725 -0.762146 -0.209268
3 -2.426604 -0.762146  0.261904
4 -0.024134       NaN  0.261904
5  1.303691       NaN  0.261904

# 평균값이나 중간값으로 채우기

In [68]: data = pd.Series([1., np.nan, 3.5, np.nan, 7])
In [71]: data
Out[71]:
0    1.0
1    NaN
2    3.5
3    NaN
4    7.0
dtype: float64

In [69]: data.fillna(data.mean()) #평균값 (NaN값은 개수로 세지 않는다)
Out[69]:
0    1.000000
1    3.833333
2    3.500000
3    3.833333
4    7.000000
dtype: float64
```
## 데이터 변형
데이터 필터링, 정제, 변형
```python
# 중복 제거

In [75]: data = pd.DataFrame({'k1':['one', 'two'] * 3 + ['two'], 'k2':[1, 1, 2, 3, 3, 4, 4]})

In [76]: data
Out[76]:
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4
6  two   4

In [77]: data.duplicated()
Out[77]:
0    False
1    False
2    False 
3    False
4    False
5    False # 인덱스6 은 아직 읽히지 않아서 False
6     True # 인덱스5와 중복
dtype: bool

In [78]: data.drop_duplicates() # duplicated 배열이 False인 즉 중복이 아닌 행으로 DF를 필터링해 반환
Out[78]:
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4

In [79]: data['v1'] = range(7) 

In [80]: data
Out[80]:
    k1  k2  v1
0  one   1   0
1  two   1   1
2  one   2   2
3  two   3   3
4  one   3   4
5  two   4   5
6  two   4   6

In [81]: data.drop_duplicates(subset=['k1'])# 배열 내 부분집합을 따로 지정할 수도 있음
Out[81]:
    k1  k2  v1
0  one   1   0
1  two   1   1 # 인덱스 2부터는 이전에 읽힌 K1열 내 값과 전부 중복이므로 제외됨

In [82]: data.drop_duplicates(['k1', 'k2'], keep='last') # keep='last' 중복시 마지막 발견된 값을 유지,반환
Out[82]:
    k1  k2  v1
0  one   1   0
1  two   1   1
2  one   2   2
3  two   3   3
4  one   3   4
6  two   4   6
```
```python
In [83]: data = pd.DataFrame({'food':['bacon', 'pulled pork', 'bacon',
    ...: 'pastrami', 'corned beef', 'bacon',
    ...: 'pastrami', 'honey ham', 'nova lox'],
    ...: 'ounces':[4, 3, 12, 6, 7.5, 8, 3, 5, 6]})

In [84]: data
Out[84]:
          food  ounces
0        bacon     4.0
1  pulled pork     3.0
2        bacon    12.0
3     pastrami     6.0
4  corned beef     7.5
5        bacon     8.0
6     pastrami     3.0
7    honey ham     5.0
8     nova lox     6.0

In [85]: meat_to_animal = {
    ...: 'bacon' : 'pig',
    ...: 'pulled pork' : 'pig',
    ...: 'pastrami' : 'cow',
    ...: 'corned beef' : 'cow',
    ...: 'honey ham' : 'pig',
    ...: 'nova lox':'salmon'
    ...: }

In [86]: data['animal'] = data['food'].map(meat_to_animal) # map매소드는 DF는 불가능하고 Series형태만 가능하다


In [87]: data
Out[87]:
          food  ounces  animal
0        bacon     4.0     pig
1  pulled pork     3.0     pig
2        bacon    12.0     pig
3     pastrami     6.0     cow
4  corned beef     7.5     cow
5        bacon     8.0     pig
6     pastrami     3.0     cow
7    honey ham     5.0     pig
8     nova lox     6.0  salmon

In [89]: def get_animal(x):
    ...:     return meat_to_animal[x]
    ...:

In [90]: data['food'].map(get_animal)
Out[90]:
0       pig
1       pig
2       pig
3       cow
4       cow
5       pig
6       cow
7       pig
8    salmon
Name: food, dtype: object
```

## 값 치환
```python
# 연습할 Series 생성
In [94]: data = pd.Series([1., -999., 2., -999., -1000., 3.])

In [95]: data
Out[95]:
0       1.0
1    -999.0
2       2.0
3    -999.0
4   -1000.0
5       3.0
dtype: float64 

# 여기서 -999를 누락된 데이터를 표시하는 감싯값이라 해보자

In [96]: data.replace(-999, np.nan)
Out[96]:
0       1.0
1       NaN
2       2.0
3       NaN
4   -1000.0
5       3.0
dtype: float64

# 여러 값을 한꺼번에 치환하려면 치환하려는 값을 리스트로 묶어 넘긴다

In [97]: data.replace([-999, -1000], np.nan)
Out[97]:
0    1.0
1    NaN
2    2.0
3    NaN
4    NaN
5    3.0
dtype: float64

# 각 값을 다른 값으로 치환하려면 새로 지정할 값의 리스트를 전달한다

In [98]: data.replace([-999, -1000], [np.nan, 0])
Out[98]:
0    1.0
1    NaN
2    2.0
3    NaN
4    0.0
5    3.0
dtype: float64

# 리스트 대신 딕셔너리를 이용할 수도 있다

In [99]: data.replace({-999: np.nan, -1000: 0})
Out[99]:
0    1.0
1    NaN
2    2.0
3    NaN
4    0.0
5    3.0
dtype: float64
```

## 축 색인 이름 바꾸기
```python
In [102]: data = pd.DataFrame(np.arange(12).reshape((3, 4)),
     ...: index = ['Ohio', 'Colorado', 'New York'],
     ...: columns = ['one', 'two', 'three', 'four'])

# In [110]: np.arange(12).reshape((3, 4)) # 배열확인해보기
# Out[110]:
# array([[ 0,  1,  2,  3],
#        [ 4,  5,  6,  7],
#        [ 8,  9, 10, 11]])


In [109]: data
Out[109]:
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
New York    8    9     10    11

In [103]: def transform(x):
     ...:     return x[:4].upper()

In [104]: data.index.map(transform)
Out[104]: Index(['OHIO', 'COLO', 'NEW '], dtype='object')

In [105]: data.index = data.index.map(transform)

In [106]: data
Out[106]:
      one  two  three  four
OHIO    0    1      2     3
COLO    4    5      6     7
NEW     8    9     10    11

# 원본 유지, 복사본에 변경사항 만들기

In [114]: data.rename(index=str.title, columns=str.upper) # rename 매서드는 원래 객체에 변경안함, 새 객체 생성
Out[114]:
      ONE  TWO  THREE  FOUR
Ohio    0    1      2     3
Colo    4    5      6     7
New     8    9     10    11

In [116]: data.rename(index={'OHIO':'INDIANA'},
     ...: columns={'three':'peekaboo'})
Out[116]:
         one  two  peekaboo  four
INDIANA    0    1         2     3
COLO       4    5         6     7
NEW        8    9        10    11
```

## 이산화
연속되는 데이터는 종종 분석을 위해 그룹으로 나누기도 한다
```python
# 학생 그룹 데이터를 나이대에 따라 분류해보기

In [117]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]

In [118]: bins = [18, 25, 35, 60, 100] # 분류할 범위 ( 18초과~25이하, 25초과~35이하 ...)

In [119]: age_categories = pd.cut(ages, bins)

In [120]: age_categories
Out[120]:
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]
Length: 12
Categories (4, interval[int64, right]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]
```
```python
In [121]: age_categories.codes
Out[121]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)

In [122]: age_categories.categories
Out[122]: IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]], dtype='interval[int64, right]')

In [123]: age_categories.categories[0]
Out[123]: Interval(18, 25, closed='right')

In [124]: pd.value_counts(age_categories)
Out[124]:
(18, 25]     5
(25, 35]     3
(35, 60]     3
(60, 100]    1
dtype: int64

In [125]: pd.cut(ages, bins, right=False)
Out[125]:
[[18, 25), [18, 25), [25, 35), [25, 35), [18, 25), ..., [25, 35), [60, 100), [35, 60), [35, 60), [25, 35)]
Length: 12
Categories (4, interval[int64, left]): [[18, 25) < [25, 35) < [35, 60) < [60, 100)]

In [126]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']

In [127]: pd.cut(ages, bins, labels=group_names)
Out[127]:
['Youth', 'Youth', 'Youth', 'YoungAdult', 'Youth', ..., 'YoungAdult', 'Senior', 'MiddleAged', 'MiddleAged', 'YoungAdult']
Length: 12
Categories (4, object): ['Youth' < 'YoungAdult' < 'MiddleAged' < 'Senior']

In [128]: data = np.random.uniform(size=20)

In [129]: data
Out[129]:
array([0.90866884, 0.06189849, 0.65529827, 0.59599602, 0.18473031,
       0.629629  , 0.84133757, 0.7509879 , 0.77877314, 0.55718968,
       0.00891716, 0.28315754, 0.99652908, 0.48992296, 0.73846918,
       0.54252   , 0.10572158, 0.56082811, 0.01957482, 0.806344  ])

In [130]: pd.cut(data, 4, precision=2) # 4등분, 소수점 아래 두자리까지
Out[130]:
[(0.75, 1.0], (0.0079, 0.26], (0.5, 0.75], (0.5, 0.75], (0.0079, 0.26], ..., (0.5, 0.75], (0.0079, 0.26], (0.5, 0.75], (0.0079, 0.26], (0.75, 1.0]]
Length: 20
Categories (4, interval[float64, right]): [(0.0079, 0.26] < (0.26, 0.5] < (0.5, 0.75] < (0.75, 1.0]]

In [131]: data = np.random.standard_normal(1000)

In [132]: quartiles = pd.qcut(data, 4, precision=2)

In [133]: quartiles
Out[133]:
[(0.055, 0.73], (0.73, 4.14], (-0.67, 0.055], (0.73, 4.14], (-0.67, 0.055], ..., (0.73, 4.14], (-3.1799999999999997, -0.67], (-0.67, 0.055], (0.055, 0.73], (-3.1799999999999997, -0.67]]
Length: 1000
Categories (4, interval[float64, right]): [(-3.1799999999999997, -0.67] < (-0.67, 0.055] < (0.055, 0.73] <
                                           (0.73, 4.14]]

In [134]: pd.value_counts(quartiles)
Out[134]:
(-3.1799999999999997, -0.67]    250
(-0.67, 0.055]                  250
(0.055, 0.73]                   250
(0.73, 4.14]                    250
dtype: int64

# pandas.cut 함수 처럼 사분위수를 직접 지정할 수 있다
# 사분위수는 0~ 1까지의 값이다
In [135]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]).value_counts()
Out[135]:
(-3.167, -1.34]    100
(-1.34, 0.0552]    400
(0.0552, 1.395]    400
(1.395, 4.142]     100
dtype: int64
```