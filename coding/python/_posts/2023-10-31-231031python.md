---
layout: single
title: "웹 크롤링"
date: "2023-10-30 10:00:00 +0900"
last_modified_at: "2023-10-30 11:00:00 +0900"
author_profile: true
sidebar:
  nav: "main"
categories:
  - Python
---

웹 크롤링 알아보기


## 웹 크롤링에 필요한 파이썬 라이브러리 Beautiful Soup, requests

Beautiful Soup 한글번역 문서 https://www.crummy.com/software/BeautifulSoup/bs4/doc.ko/

HTML과 XML(확장된 HTML) 파일로부터 데이터를 뽑아내기 위한 파이썬 라이브러리이다.
requests로 소스코드를 뽑아오면 beautiful soup으로 인간이 알기쉽게 html형식으로 바꿔준다

requests 문서 https://pypi.org/project/requests/

Requests는 간단하면서도 우아한 HTTP 라이브러리입니다.

## 웹 크롤링 해보기
기상청웹 자료 크롤링<br>
![mainart](\assets\post image/231031\1.mainart.jpg)<br>

## 예제 : 기상청웹 자료 크롤링

![mainart](\assets\post image/231031\2.table.png)<br>
크롤링 대상<br><br>
![mainart](\assets\post image/231031\3.atag.png)<br>
지역은 a태그로 되어 있다<br><br>
![mainart](\assets\post image/231031\4.temp.png)<br>
![mainart](\assets\post image/231031\5.humi.png)<br>
기온과 습도는 td태그에 있다<br><br>
![mainart](\assets\post image/231031\6.tableclass.png)<br>
크롤링할 기상 테이블의 클래스 명<br><br>

```python
# 파이썬 터미널에서 pip install requests, pip install bs4 로 패키지설치 해줘야 함 (주피터노트북에선 원래 포함되어있어서 안해도 됨)
# 아래는 주피터노트북 환경이니  상관없음

import requests
from bs4 import BeautifulSoup

# 웹 페이지를 가져온 뒤 BeautifulSoup 객체로 만듦
response = requests.get('https://pythondojang.bitbucket.io/weather/observation/currentweather.html') #requests객체의 get함수. 인간이 알기어려운 형태의 소스코드가져옴
soup = BeautifulSoup(response.content, 'html.parser') # 가져온 소스코드를 html형식으로 담음

# <table class = 'table_develop3'>을 찾음
table = soup.find('table', {'class': 'table_develop3'}) # <table class="table_develop3"> 을 찾음
data = [] # 데이터 저장할 리스트 마련
for tr in table.find_all('tr'): # 모든 <tr>태그 찾기 반복 # tr은 테이블의 각 행임
                                 # 반복문으로 각 행(tr)마다 진행됨
    tds = list(tr.find_all('td')) # 그 행의 열(td) 들을 tds리스트로 만듦
    
    for td in tds:
        if td.find('a'):
            point = td.find('a').text # <a> 태그 안에서 지역을 가져옴
            temperature = tds[5].text # <td> 태그 리스트의 6번째(인덱스 5)에서 습도 가져옴
            
            humidity = tds[9].text # <td> 태그 리스트의 10번째(인덱스 9)에서 습도 가져옴
            
            data.append([point, temperature, humidity])
            
print( data )

[['서울', '25.6', '30'], ['백령도', '18.4', '62'], ['인천', '20.8', '54'], ['수원', '25.0', '41'], ['동두천', '24.9', '34'], ['파주', '25.1', '39'], ['강화', '20.0', '56'], ['양평', '25.5', '32'], ['이천', '25.6', '28'], ['북춘천', '24.6', '36'], ['북강릉', '19.9', '56'], ...


print( response )

<Response [200]> # 연결됐다는 의미 200

print ( soup )

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html lang="ko" xml:lang="ko" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>도시별 현재날씨 &gt; 지상관측자료 &gt; 관측자료 &gt; 날씨 &gt; 기상청 </title>
<link href="http://www.kma.go.kr/favicon2.ico" rel="shortcut icon"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<link href="/share/css/import.css?20160530" rel="stylesheet" type="text/css"/>
<script src="/share/js/jquery-1.7.1.min.js" type="text/javascript"></script>
<script src="/share/js/common.js?ver=20150417" type="text/javascript"></script>
...
```
## 크롤링한 자료들 메모장에 저장하기
```python
with open('weather.csv', 'w') as file:
    file.write('point, themperature, humidity\n')
    for i in data:
        file.write('{0}, {1}, {2}\n'.format(i[0], i[1], i[2]))
```
weather.csv<br>
![mainart](\assets\post image/231031\7.storedn.png)<br>

## DataFrame 객체로 만들기
csv파일을 읽고 pandas의 DataFrame 객체 만들기
```python
%matplotlib inline # 인라인 설정하면 matplotlib.pyplot의 show 함수 호출안해도 주피터노트북 안에서 그래프가 표시됨
import pandas as pd # 데이터를 저장 및 처리하는 패키지
import matplotlib as mql # 그래프를 그리는 패키지
import matplotlib.pyplot as plt # 그래프를 그리는 패키지

df = pd.read_csv('weather.csv', index_col='point', encoding='euc-kr') # csv파일을 읽어서 DataFrame 객체로 만듦, 인덱스 컬럼은 point로 설정, csv파일이 euc-kr로 인코딩되어 저장됐으니 pd.read_csv 함수에 추가해야함
df
```
결과<br>
![mainart](\assets\post image/231031\8.df.png)<br>

## 일부 데이터만 가져오는 방법
위 결과를 보다시피 지점이 너무 많아 원활한 그래프를 그리기 위해서 필요한 요소만 가져오자<br>
특별시와 광역시만 모아서 DataFrame 객체를 만들자<br>
```python
# 지점이 너무 많아 원활한 그래프를 그리기 위해  특별시, 광역시만 모아서 DataFrame 객체로 만듦
city_df = df.loc[['서울', '인천', '대구', '광주', '부산', '울산']]
city_df
```
## 그래프로 그리기
크롤링한 데이터를 바탕으로 그래프로 표현하자
```python
# windows 한글 폰트 설정. matplotlib은 기본적으로 그래프에 한글이 표시가 안되기 때문에 맑은굴림폰트 가져옴
font_name = mql.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name() # get_name()으로 해당 폰트파일의 폰트이름을 얻는다
mql.rc('font', family=font_name) # 가져온 '맑은굴림'이름의 폰트 설정

ax = city_df.plot(kind='bar', title='날씨', figsize=(12, 4), legend=True, fontsize=12)
ax.set_xlabel('도시', fontsize=12) # x축 정보 표시
ax.set_ylabel('기온/습도', fontsize=12) # y축 정보 표시
ax.legend(['기온', '습도'], fontsize=12) # 범례 지정 ( 범례는 차트의 각 막대가 무슨 값인지 표시해줌, 리스트형태 )
```
![mainart](\assets\post image/231031\9.graph.png)<br><br>

참고로 주피터 노트북에선 포함되어 있어 안해도 되지만 일반 파이썬 터미널에서는 pip install pandas, pip install matplotlib로 패키지를 설치 해줘야 함